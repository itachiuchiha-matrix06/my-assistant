import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders import UnstructuredWordDocumentLoader
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.schema.runnable import RunnableSequence
from langchain_groq import ChatGroq   
from langchain.output_parsers import StructuredOutputParser
from operator import itemgetter
import os
import tempfile


# -------------------- Page Configuration --------------------
st.set_page_config(page_title="Smart Academic Assistant", layout="centered")

# -------------------- Title --------------------
st.title("ðŸ“š Smart Academic Assistant")
st.write("Upload your academic documents and ask questions to get structured answers.")

# -------------------- File Upload Section --------------------
uploaded_files = st.file_uploader(
    "Upload academic documents (PDF, DOCX, or TXT):",
    type=["pdf", "docx", "txt"],
    accept_multiple_files=True
)

# -------------------- Question Input --------------------
question = st.text_input("Enter your academic question:")

# -------------------- Submit Button --------------------
if st.button("Get Answer"):
    if not uploaded_files or not question:
        st.warning("Please upload at least one document and enter a question.")
    else:
        # -------------------- PLACEHOLDER: RAG Pipeline Logic --------------------
        # TODO:
        
        all_documents=[]


        # 1. Load documents using LangChain document loaders
        
        for get_document in uploaded_files:
            suffix = os.path.splitext(get_document.name)[1]
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_file.write(get_document.read())
                tmp_file_path = tmp_file.name

            if get_document.name.endswith(".pdf"):
                    pdf_loader = PyPDFLoader(tmp_file_path)
                    st.write(f"Detected PDF file: {get_document.name}")


            elif get_document.name.endswith(".docx"):
                pdf_loader = UnstructuredWordDocumentLoader(tmp_file_path)
                st.write("Detected a DOCS file.")

            elif get_document.name.endswith(".txt"):
                pdf_loader = TextLoader(tmp_file_path)
                st.write("Detected a TXT file.")

            else:
                st.write("Unknown file type.")                                               #there can be multiple files uploaded so to load documents i have use for loop
            documents = pdf_loader.load()                              
            all_documents.extend(documents)
                
        # 2. Split documents using RecursiveCharacterTextSplitter or similar
        text_splitter = RecursiveCharacterTextSplitter(                          #using recursive text spliter       i have take this spliter from official langchain documnetation
            chunk_size=100,
            chunk_overlap=20,
            length_function=len,
            is_separator_regex=False,
        )
        chunks = text_splitter.split_documents(all_documents)
        


        # 3. Create embeddings and store in vector store (e.g., FAISS, Chroma)
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

        vector_store = FAISS.from_documents(
            embedding = embeddings,
            documents = chunks 
        )

        # 4. Retrieve relevant chunks based on the question
        retriever = vector_store.as_retriever(search_kwargs={"k":2})
        results = retriever.invoke(question)

        source_docs = [doc.metadata.get("source", "Unknown") for doc in results]

        # for i,doc in enumerate(results):
        #     print(f"\n---Result {i+1}---")
        #     print(doc.page_content)



        # 5. Use Groq-hosted LLM via LangChain (e.g., Mixtral, Gemma, Llama3)
        llm = ChatGroq(
        api_key=os.getenv("GROQ_API_KEY"),
        model="meta-llama/llama-4-scout-17b-16e-instruct"
        )
        # 6. Use Output Parser to format structured response
        schemas = [
            ResponseSchema(name="question", description="The input question"),
            ResponseSchema(name="answer", description="The answer generated by the LLM"),
            ResponseSchema(name="source_document", description="Source file of the information"),
            ResponseSchema(name="confidence_score", description="Model's confidence percentage")
        ]
        output_parser = StructuredOutputParser.from_response_schemas(schemas)
        format_instructions = output_parser.get_format_instructions()

        context = "\n".join([doc.page_content for doc in results])

        # Example output format (replace this with actual output):
        # response = {
        #     "question": question,
        #     "answer": "Your answer here",
        #     "source_document": "Document Name",
        #     "confidence_score": "0.93"
        # }

        schemas = [
            ResponseSchema(name="question", description="The input question"),
            ResponseSchema(name="answer", description="The answer generated by the LLM"),
            ResponseSchema(name="source_document", description="Source file of the information"),
            ResponseSchema(name="confidence_score", description="Model's confidence percentage")
        ]
        output_parser = StructuredOutputParser.from_response_schemas(schemas)
        format_instructions = output_parser.get_format_instructions()

        prompt = f"""
                You are an academic assistant. Use the following context to answer the question.
                Only answer based on the context provided.

            Context:{context}

            Question: {question}

            {format_instructions}"""


        raw_response = llm.invoke(prompt)
        try:
            structured_response = output_parser.parse(raw_response.content)

        except Exception as e:
            structured_response = {
                "question": question,
                "answer": "Could not parse structured JSON response.",
                "source_document": source_docs[0] if source_docs else "Unknown",
                "confidence_score": "N/A",
                "error": str(e),
                "raw_response": raw_response
            }
        #
        st.subheader("ðŸ“„ Answer:")
        st.json(structured_response)

        st.info("Implement your RAG logic above and display the final structured response here.")

# -------------------- Bonus Section: Agent Tools --------------------
st.markdown("---")


